{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Trabajo Pracatico 4 - Redes Neuronales Convolucionales\n",
    "\n",
    "Vamos a realizar una clasificacion binaria de imagnes que pueden ser de gatos o perros, para ello vamos a utilizar un dataset de Kaggle llamado \"cats-vs-dogs\" que contiene 23.409 imágenes de gatos y perros. El objetivo es entrenar un modelo de clasificación binaria que pueda distinguir entre imágenes de gatos y perros.\n",
    "\n",
    "Se van a proponer los siguientes modelos:\n",
    "\n",
    "- **Modelo 1:** Red convolucional simple (SimpleCNN) creada desde cero.\n",
    "- **Modelo 2:** Red convolucional basada en ResNet18.\n",
    "- **Modelo 4:** Red convolucional avanzacda (AdvancedCNN) creada desde cero.\n",
    "- **Modelo 3:** Red convolucional basada en ResNet18 con cambios en hiperparametros.\n",
    "- **Modelo 5:** Red convolucional basada en Inception de Google con cambios en hiperparametros."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5aa4d9bed7ac6d10"
   },
   "id": "5aa4d9bed7ac6d10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Asignamos el dataset a la variable **dataset**"
   ],
   "metadata": {
    "id": "_1gUgAWK1Rqf"
   },
   "id": "_1gUgAWK1Rqf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cats_vs_dogs\")"
   ],
   "metadata": {
    "id": "62a3f5b0b4d5fd4e",
    "outputId": "15f9cbac-d21c-42b2-b66f-c2e0559427bd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:11.526658Z",
     "start_time": "2024-10-16T02:43:44.007696Z"
    }
   },
   "id": "62a3f5b0b4d5fd4e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos un *DataFrame* llamado **mydataset**, el cual almacenará el path de cada imágen junto a su etiqueta (perro o gato). Además creamos un directorio llamado dataset y almacenamos allí las imágenes.\n"
   ],
   "metadata": {
    "id": "jiS_pLhO1hIe"
   },
   "id": "jiS_pLhO1hIe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "             image_path label\n0  ./dataset/img_0.jpeg     0\n1  ./dataset/img_1.jpeg     0\n2  ./dataset/img_2.jpeg     0\n3  ./dataset/img_3.jpeg     0\n4  ./dataset/img_4.jpeg     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./dataset/img_0.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./dataset/img_1.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./dataset/img_2.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./dataset/img_3.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./dataset/img_4.jpeg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "main_dir = './dataset'\n",
    "os.makedirs(main_dir, exist_ok=True)\n",
    "\n",
    "mydataset = pd.DataFrame(columns=['image_path', 'label'])\n",
    "\n",
    "for i in range(len(dataset['train'])):\n",
    "    img_path = f\"{main_dir}/img_{i}.jpeg\"\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        dataset['train'][i]['image'].save(img_path)\n",
    "\n",
    "    mydataset.at[i, 'image_path'] = img_path\n",
    "    mydataset.at[i, 'label'] = dataset['train'][i]['labels']\n",
    "\n",
    "mydataset.head()"
   ],
   "metadata": {
    "id": "18f786f3d1d77ead",
    "outputId": "dc442974-49e8-4411-f62a-1fb94012e8fc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:31.993621Z",
     "start_time": "2024-10-16T02:55:11.527749Z"
    }
   },
   "id": "18f786f3d1d77ead",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos un diccionario para almacenar los parámetros que usaremos."
   ],
   "metadata": {
    "collapsed": false,
    "id": "b74f536a3a8038ee"
   },
   "id": "b74f536a3a8038ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exp_config = dict()"
   ],
   "metadata": {
    "id": "efa97dd716f5447a",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:31.995834Z",
     "start_time": "2024-10-16T02:55:31.994281Z"
    }
   },
   "id": "efa97dd716f5447a",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos la semilla para que al divir el dataset en train, test y val, sea siempre la misma división de datos. Además, especificamos la proporción de datos que serán para testeo y para validación."
   ],
   "metadata": {
    "id": "VKScy5ZE1_KS"
   },
   "id": "VKScy5ZE1_KS"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seed = 42\n",
    "test_size = 0.15\n",
    "val_size = 0.20\n",
    "\n",
    "exp_config['seed'] = seed\n",
    "exp_config['test_size'] = test_size\n",
    "exp_config['val_size'] = val_size"
   ],
   "metadata": {
    "id": "5ae023464ee7d8ff",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:31.998521Z",
     "start_time": "2024-10-16T02:55:31.997003Z"
    }
   },
   "id": "5ae023464ee7d8ff",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dividimos el dataset en *train*, *test*, *val*.\n",
    "\n",
    "**Aclaración:** los datos de validación surgen de una parte de los datos de testeo."
   ],
   "metadata": {
    "id": "wSXflM8R23IA"
   },
   "id": "wSXflM8R23IA"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(mydataset, test_size=test_size, stratify=mydataset['label'], random_state=seed)\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size, stratify=train_val_df['label'], random_state=seed)"
   ],
   "metadata": {
    "id": "acfdebddb8975812",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:32.869418Z",
     "start_time": "2024-10-16T02:55:31.999014Z"
    }
   },
   "id": "acfdebddb8975812",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Añadimos parámetros de configuración al diccionario."
   ],
   "metadata": {
    "id": "ZZKi7exX3C1O"
   },
   "id": "ZZKi7exX3C1O"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "exp_config['train_n_cats'] = train_df['label'].value_counts()[0]\n",
    "exp_config['train_n_dogs'] = train_df['label'].value_counts()[1]\n",
    "exp_config['val_n_cats'] = val_df['label'].value_counts()[0]\n",
    "exp_config['val_n_dogs'] = val_df['label'].value_counts()[1]\n",
    "exp_config['test_n_cats'] = test_df['label'].value_counts()[0]\n",
    "exp_config['test_n_dogs'] = test_df['label'].value_counts()[1]"
   ],
   "metadata": {
    "id": "917c26bc02be395f",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:32.874018Z",
     "start_time": "2024-10-16T02:55:32.870008Z"
    }
   },
   "id": "917c26bc02be395f",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "La clase **CatsDogsDataset** es una implementación personalizada de una clase llamda *Dataset* de PyTorch que permite cargar y transformar las imágenes del dataset.\n",
    "\n",
    "**Explicación**\n",
    "1. Constructor (\\_\\_init\\_\\_):  \n",
    "- img_path_list: Lista de rutas de las imágenes.\n",
    "- lab_list: Lista de etiquetas correspondientes a las imágenes (0 para gatos, 1 para perros).\n",
    "- transform: Transformaciones opcionales que se aplicarán a las imágenes (por ejemplo, redimensionar, normalizar).\n",
    "2. Método \\_\\_len\\_\\_:  \n",
    "- Devuelve la cantidad de imágenes en el conjunto de datos.\n",
    "3. Método \\_\\_getitem\\_\\_:\n",
    "- idx: Índice de la imagen y etiqueta que se desea obtener.\n",
    "- img_path: Obtiene la ruta de la imagen en el índice idx.\n",
    "- image: Abre la imagen y la convierte a formato RGB.\n",
    "- label: Obtiene la etiqueta correspondiente a la imagen y la convierte a un tensor de PyTorch.\n",
    "- Si se especificaron transformaciones, se aplican a la imagen.\n",
    "- Devuelve la imagen transformada y su etiqueta correspondiente."
   ],
   "metadata": {
    "id": "c-1bZ5CU3Gri"
   },
   "id": "c-1bZ5CU3Gri"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, img_path_list, lab_list, transform=None):\n",
    "        self.transform = transform\n",
    "        self.images = img_path_list\n",
    "        self.labels = lab_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        label = torch.Tensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "id": "2cd7f71cfa333705",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:33.886697Z",
     "start_time": "2024-10-16T02:55:32.874552Z"
    }
   },
   "id": "2cd7f71cfa333705",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos la resolución de las imágenes que serán procesadas."
   ],
   "metadata": {
    "id": "mNXY7zTh31QJ"
   },
   "id": "mNXY7zTh31QJ"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_size = (224,224)\n",
    "exp_config['input_size'] = input_size"
   ],
   "metadata": {
    "id": "61b41a6d728d3c10",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:33.888785Z",
     "start_time": "2024-10-16T02:55:33.887348Z"
    }
   },
   "id": "61b41a6d728d3c10",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como las imágenes son a color en formato RGB, definiremos 3 canales"
   ],
   "metadata": {
    "id": "fBTkFhnn382u"
   },
   "id": "fBTkFhnn382u"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_channels = 3\n",
    "exp_config['n_channels'] = n_channels"
   ],
   "metadata": {
    "id": "7e9634b1fc3514d6",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:33.890662Z",
     "start_time": "2024-10-16T02:55:33.889442Z"
    }
   },
   "id": "7e9634b1fc3514d6",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos el *transform* que será usado, el cual redimensiona las imágenes a la resolución dada."
   ],
   "metadata": {
    "id": "qb6wxASF4KHR"
   },
   "id": "qb6wxASF4KHR"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <ABB5F068-9559-30AA-9C1B-E6B23C0D4289> /opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/PracticaIA/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/PracticaIA/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ],
   "metadata": {
    "id": "60979fd3aadf9c30",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:34.641596Z",
     "start_time": "2024-10-16T02:55:33.892366Z"
    }
   },
   "id": "60979fd3aadf9c30",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos los datasets de train, test y val."
   ],
   "metadata": {
    "id": "j-RUgCbO4Yg_"
   },
   "id": "j-RUgCbO4Yg_"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataset = CatsDogsDataset(train_df['image_path'].tolist(), train_df['label'].tolist(), transform)\n",
    "test_dataset = CatsDogsDataset(test_df['image_path'].tolist(), test_df['label'].tolist(), transform)\n",
    "val_dataset = CatsDogsDataset(val_df['image_path'].tolist(), val_df['label'].tolist(), transform)"
   ],
   "metadata": {
    "id": "5adde669c2867e8d",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:34.644536Z",
     "start_time": "2024-10-16T02:55:34.642284Z"
    }
   },
   "id": "5adde669c2867e8d",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos los *DataLoaders* de train, test y val, y definimos el tamaño de lote.\n",
    "\n",
    "**Aclaración:** el batch size de test es 1,los datos no serán mezclados por cada época y no se eliminarán datos para alcanzar el tamaño de lote establecido."
   ],
   "metadata": {
    "id": "CzsTv8jC4-Js"
   },
   "id": "CzsTv8jC4-Js"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "exp_config['batch_size'] = batch_size\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=False)"
   ],
   "metadata": {
    "id": "cc5bed8c2518e66c",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:34.647255Z",
     "start_time": "2024-10-16T02:55:34.645109Z"
    }
   },
   "id": "cc5bed8c2518e66c",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WandB"
   ],
   "metadata": {
    "id": "EIOjZvGA-7hH"
   },
   "id": "EIOjZvGA-7hH"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mintart-estudiantes\u001B[0m (\u001B[33mar-um\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/francobertoldi/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=\"d567fa512c6502cc7986d8c90fd37c4f0969de0d\")"
   ],
   "metadata": {
    "id": "xdsh8Yex_rmJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32355bbe-d244-45e8-99aa-f6339a400734",
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:36.599653Z",
     "start_time": "2024-10-16T02:55:34.647817Z"
    }
   },
   "id": "xdsh8Yex_rmJ",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Uso de los CNNs"
   ],
   "metadata": {
    "collapsed": false,
    "id": "deab16e8e1c742a1"
   },
   "id": "deab16e8e1c742a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tools.train_val_test import run\n",
    "from tools.models import SimpleCNN, ResNet18, AdvancedCNN, DenseNet_121CNN"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:55:36.615633Z",
     "start_time": "2024-10-16T02:55:36.602260Z"
    }
   },
   "id": "94938127f22b79ea",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SimpleCNN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a5ccc5191154960e"
   },
   "id": "a5ccc5191154960e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:0inasarj) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "560b1763b7d0465db5677a34627d52fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">Bertoldi_Mancuso_SimpleCNN</strong> at: <a href='https://wandb.ai/ar-um/CNN_CatsvsDogs/runs/0inasarj' target=\"_blank\">https://wandb.ai/ar-um/CNN_CatsvsDogs/runs/0inasarj</a><br/> View project at: <a href='https://wandb.ai/ar-um/CNN_CatsvsDogs' target=\"_blank\">https://wandb.ai/ar-um/CNN_CatsvsDogs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241015_235536-0inasarj/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:0inasarj). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/francobertoldi/Library/CloudStorage/GoogleDrive-f.bertoldi@alumno.um.edu.ar/Mi unidad/Facultad/4to año/2do Semestre/Inteligencia Artificial/repo/TP4/wandb/run-20241016_000129-4f23ufh7</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/ar-um/CNN_CatsvsDogs/runs/4f23ufh7' target=\"_blank\">Bertoldi_Mancuso_SimpleCNN</a></strong> to <a href='https://wandb.ai/ar-um/CNN_CatsvsDogs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/ar-um/CNN_CatsvsDogs' target=\"_blank\">https://wandb.ai/ar-um/CNN_CatsvsDogs</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/ar-um/CNN_CatsvsDogs/runs/4f23ufh7' target=\"_blank\">https://wandb.ai/ar-um/CNN_CatsvsDogs/runs/4f23ufh7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n",
      " SHAPE: torch.Size([64, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mBCELoss()\n\u001B[1;32m      8\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexp_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/GoogleDrive-f.bertoldi@alumno.um.edu.ar/Mi unidad/Facultad/4to año/2do Semestre/Inteligencia Artificial/repo/TP4/tools/train_val_test.py:30\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer, num_epochs)\u001B[0m\n\u001B[1;32m     26\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(base_checkpoint_path, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     28\u001B[0m checkpoint_path \u001B[38;5;241m=\u001B[39m base_checkpoint_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/best_model\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m model_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 30\u001B[0m \u001B[43mtrain_and_validate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_patience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     35\u001B[0m y_true, y_proba \u001B[38;5;241m=\u001B[39m test(model, test_dataloader, device)\n",
      "File \u001B[0;32m~/Library/CloudStorage/GoogleDrive-f.bertoldi@alumno.um.edu.ar/Mi unidad/Facultad/4to año/2do Semestre/Inteligencia Artificial/repo/TP4/tools/utils.py:73\u001B[0m, in \u001B[0;36mtrain_and_validate\u001B[0;34m(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs, early_stopping_patience, checkpoint_path)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     72\u001B[0m     train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m train(model, train_dataloader, criterion, optimizer, device)\n\u001B[0;32m---> 73\u001B[0m     val_loss, val_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m], \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     76\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_accuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     77\u001B[0m           \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mValidation Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Validation Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_accuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     79\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m: epoch,\n\u001B[1;32m     80\u001B[0m                \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m: train_accuracy,\n\u001B[1;32m     81\u001B[0m                \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: train_loss,\n\u001B[1;32m     82\u001B[0m                \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_acc\u001B[39m\u001B[38;5;124m\"\u001B[39m: val_accuracy,\n\u001B[1;32m     83\u001B[0m                \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m: val_loss})\n",
      "File \u001B[0;32m~/Library/CloudStorage/GoogleDrive-f.bertoldi@alumno.um.edu.ar/Mi unidad/Facultad/4to año/2do Semestre/Inteligencia Artificial/repo/TP4/tools/utils.py:49\u001B[0m, in \u001B[0;36mvalidate\u001B[0;34m(model, val_dataloader, criterion, device)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m val_dataloader:\n\u001B[1;32m     47\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 49\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     52\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/CloudStorage/GoogleDrive-f.bertoldi@alumno.um.edu.ar/Mi unidad/Facultad/4to año/2do Semestre/Inteligencia Artificial/repo/TP4/tools/models.py:22\u001B[0m, in \u001B[0;36mSimpleCNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     20\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)\n\u001B[1;32m     21\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[0;32m---> 22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)\n\u001B[1;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/modules/pooling.py:213\u001B[0m, in \u001B[0;36mMaxPool2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor):\n\u001B[0;32m--> 213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    216\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mceil_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/_jit_internal.py:624\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 624\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mif_false\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/PracticaIA/lib/python3.11/site-packages/torch/nn/functional.py:830\u001B[0m, in \u001B[0;36m_max_pool2d\u001B[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[0m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     stride \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mannotate(List[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[0;32m--> 830\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "id": "VjEFHNvs-Mvh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "outputId": "bfbcda04-e344-4609-911c-f3989519154e",
    "ExecuteTime": {
     "end_time": "2024-10-16T03:04:37.505963Z",
     "start_time": "2024-10-16T03:01:29.896018Z"
    }
   },
   "id": "VjEFHNvs-Mvh",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet18"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d60bce6a14c24be5"
   },
   "id": "d60bce6a14c24be5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "model = ResNet18()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:59:44.914251Z",
     "start_time": "2024-10-16T02:59:44.914205Z"
    }
   },
   "id": "5356034fb2e45f05"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet 18 Modificado"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e747e4c92c04dfa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "model = ResNet18()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T02:59:44.914848Z",
     "start_time": "2024-10-16T02:59:44.914770Z"
    }
   },
   "id": "2d6dd313b75ea086"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AdvancedCNN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "345abf7483b2b9ce"
   },
   "id": "345abf7483b2b9ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# \n",
    "# model = AdvancedCNN()\n",
    "# \n",
    "# criterion = nn.BCELoss()\n",
    "# \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# \n",
    "# run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4794797822ab9b3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DenseNet_121CNN"
   ],
   "metadata": {
    "collapsed": false,
    "id": "70b0e1855091aa65"
   },
   "id": "70b0e1855091aa65"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "model = DenseNet_121CNN()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2eeecbca3d1724"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DenseNet_121 Modificado"
   ],
   "metadata": {
    "collapsed": false,
    "id": "189f456affd6af9f"
   },
   "id": "189f456affd6af9f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# \n",
    "# model = DenseNet_121CNN()\n",
    "# \n",
    "# criterion = nn.BCELoss()\n",
    "# \n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# \n",
    "# run(exp_config, train_dataloader, val_dataloader, test_dataloader, model, criterion, optimizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5c7f960bf3fa42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9e3071e8ea904846964d018eab0e86f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbc769cb31424b1e8da897b3099f1dfb",
       "IPY_MODEL_5476b33855e24e77922dd1b9f92b2748"
      ],
      "layout": "IPY_MODEL_b4c2aa69bd584dec8b7774f753d49812"
     }
    },
    "bbc769cb31424b1e8da897b3099f1dfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4dc9890ebad4cca8e9d48d72c1677e1",
      "placeholder": "​",
      "style": "IPY_MODEL_10ef05e6b391417aa12903ae5f977a1c",
      "value": "0.015 MB of 0.015 MB uploaded\r"
     }
    },
    "5476b33855e24e77922dd1b9f92b2748": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30f8afd4bca94b98a47c8934b3d70c41",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e280dcac892c4ff6ba552c7243e26c20",
      "value": 1
     }
    },
    "b4c2aa69bd584dec8b7774f753d49812": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4dc9890ebad4cca8e9d48d72c1677e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10ef05e6b391417aa12903ae5f977a1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30f8afd4bca94b98a47c8934b3d70c41": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e280dcac892c4ff6ba552c7243e26c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84ec112a9c174c939fe250a9083543b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_935b83a955cb42d19579e03f491dfb45",
       "IPY_MODEL_8d2b17835b5b4d77a1f5e0215876cc96"
      ],
      "layout": "IPY_MODEL_bbfd635ae8ac423bb97797d9c8b7e141"
     }
    },
    "935b83a955cb42d19579e03f491dfb45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d84db8e96a6842b3b8f5ae48317ebd44",
      "placeholder": "​",
      "style": "IPY_MODEL_3d8bac83363c442ab0a1e5fbddb27a3b",
      "value": "0.015 MB of 0.015 MB uploaded\r"
     }
    },
    "8d2b17835b5b4d77a1f5e0215876cc96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2dddd4cd31ad4ee99ba8e8bc569b5505",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17cc6ec5102849ccadf9d5fb8ab68e1f",
      "value": 1
     }
    },
    "bbfd635ae8ac423bb97797d9c8b7e141": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d84db8e96a6842b3b8f5ae48317ebd44": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d8bac83363c442ab0a1e5fbddb27a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2dddd4cd31ad4ee99ba8e8bc569b5505": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17cc6ec5102849ccadf9d5fb8ab68e1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
